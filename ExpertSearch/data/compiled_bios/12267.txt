Edgar Dobriban Statistics Department Skip to content Skip to main menu Faculty & Research Youth Program Undergrad MBA EMBA PhD Executive Education Wharton Online Alumni Search Wharton Mobile menu toggle Statistics Department Menu Home Faculty Faculty List Faculty Awards Recruiting Energy Analytics, Causal Inference, and Machine Learning Postdoc Research Research Papers / Publications Research Centers Wharton Seminars / Conferences Previous Statistics Seminars Related Seminars Programs Undergraduate Program Undergraduate Contact Information Undergraduate Statistics Concentration Undergraduate Statistics Minor Business Analytics Joint Concentration Undergraduate Course Descriptions Undergraduate Course Schedule Undergraduate Employment and Educational Opportunities MBA Program MBA Contact Information MBA Major in Statistics MBA Major in Business Analytics MBA Major in Actuarial Science MBA Course Descriptions MBA Course Schedule PhD Program PhD Contact Information PhD Curriculum PhD Course Descriptions PhD Course Schedule Current PhD Students Prospective PhD Students Apply to Wharton Financial Aid Dual Masters Degree in Statistics The Actuarial Science Program Actuarial Contact Information Reimbursement of Exam Fees, Library of Study Manuals Professional Examinations and Actuarial Courses at Wharton Course Requirements: Actuarial Science Concentration Course Requirements: University Minor in Actuarial Mathematics Course Requirements: MBA Major in Actuarial Science Prizes, Awards, and Scholarships for Actuarial Students Penn Actuarial Society Executive Education Resources Department Information Faculty PhD Students Postdoctoral Researchers Administrative Staff Contact Us Directions / Office Map Find an Expert Edgar Dobriban Assistant Professor of Statistics Contact Information Primary Email: dobriban@wharton.upenn.edu office Address: 465 Jon M. Huntsman Hall3730 Walnut StreetPhiladelphia, PA 19104 Research Interests: Statistics and machine learning Overview Research Teaching Awards and Honors Misc Overview The two main interests in my group are: the efficient statistical analysis of big data using advanced tools, such as those from random matrix theory PCA: [1], [2], [3], [4], [5], [6] multiple testing: [1], [2], [3] high-dimensional regression: [1], [2] theoretical analysis of modern machine learning, including deep learning data augmentation weight normalization stochastic gradient descent and flow overparametrization sketching and random projections, [1], [2], [3], [4], [5] distributed learning: [1], [2] adversarial robustness retraining of ML models The group is always looking to expand. We are recruiting PhD students at Penn to work on problems in statistics and machine learning. PhD applicants interested to work with me should mention this on their application. Please apply through both the Statistics department and the AMCS program, as it gives higher chances for admission. Seminar class in Fall 2019: Topics in Deep Learning (STAT-991), surveying advanced topics in deep learning research based on student presentations. See the Github page for the class materials. Education (cv): PhD in Statistics, Stanford University, 2017. Advisor:David Donoho BA in Mathematics, Princeton University, 2012. Recent news: Dec 2020: New paper on selecting the number of components in PCA via random signflips with David Hong and Yue Sheng Nov 2020: New paper on sparse sketches with small inversion bias with Micha Dereziski, Zhenyu Liao, and Michael W. Mahoney Oct 2020: Co-organizing session on Open problems in robustness at the CDTI Workshop on The Analytical Foundations of Deep Learning. Oct 23, 12pm-1:30pm. Join via Zoom or YouTube! Oct 2020: New paper: What causes the test error? Going beyond bias-variance via ANOVA, with Licong Lin Sept 2020: Three papers accepted at NeurIPS 2020: A Group-Theoretic Framework for Data Augmentation (oral presentation, 1% of papers selected), Implicit Regularization and Convergence of Weight Normalization, and Optimal Iterative Sketching with the Subsampled Randomized Hadamard Transform Sept 2020: Co-organizing online workshop on Equivariance and Data Augmentation with Kostas Daniilidis, Sept 4. Join us! Part of activities for NSF Tripods center FinPenn. Youtube playlist. Aug 2020: Edgar is a Co-PI on NSF-Simons Mathematical and Scientific Foundations of Deep Learning award THEORINET (PI: Rene Vidal, JHU). Looking forward to many exciting collaborations that this will enable! Aug 2020: Talks at JSM 2020: See slides for talk on stochastic gradient flow, and slides prepared as a discussant for the invited session on the theory of deep learning. June 2020: SIAM Mathematics of Data Science minisymposium High-Dimensional PCA in the High-Noise Regime co-organized with William Leeb and Amit Singer. Link available from the SIAM webpage closer to the date. Please check the spreadsheet linked under You can find the schedule of these talks here. June 2020: New paper on provable tradeoffs in adversarially robust classification with Hamed Hassani, David Hong, Alexander Robey June 2020: Paper on distributed learning with Yue Sheng accepted at the Annals of Statistics. June 2020: Papers accepted at ICML 2020: WONDER: Weighted one-shot distributed ridge regression (short version), The Implicit Regularization of Stochastic Gradient Flow for Least Squares, and DeltaGrad: Rapid retraining of machine learning models (with Yinjun Wu and Susan Davidson). May 2020: Paper on How to reduce dimension with PCA and random projections? with Fan Yang, Sifan Liu, and David P. Woodruff. Apr 2020: Paper on WONDER: Weighted one-shot distributed ridge regression with Yue Sheng accepted to JMLR. Mar 2020: New paper on stochastic gradient descent and flow with Alnur Ali and Ryan Tibshirani. Feb 2020: New paper on iterative sketching with Jonathan Lacotte, Sifan Liu, and Mert Pilanci. Feb 2020: Talk at UMass Amherst (cancelled due to Covid-19). Jan 2020: Edgar becomes an affiliate of the Warren Center. Dec 2019: Edgar will be teaching Continuing Education course Machine Learning and Deep Learning, with Annie Qu and Xiao Wang, at the 2020 Joint Statistical Meetings (JSM). Also teaching short course Deep Learning in Statistics at SLDS 2020. Dec 2019: Ridge regression paper with Sifan Liu accepted as a spotlight presentation to ICLR 2020 (top 6% of papers). Dec 2019: New GPU machines arrive. Nov 2019: New paper on Implicit Regularization of Normalization Methods Oct 2019: Reading group/group meeting starts. Sept-Nov 2019: Edgar is visiting the IAS for the Special year on Statistics, Machine Learning, and Optimization. Sept 2019: PCA paper accepted to the Annals of Statistics Sept 2019: Multiple testing paper accepted to Biometrika Sept 2019: Sketching paper accepted to NeurIPS 2019 Aug 2019: Edgar contributes to ASA statement Comment on Statistics, as a Discipline and Practice, in AI Research and Development Aug 2019: David Hong joins the group. Welcome David! July 2019: NSF TRIPODS awarded to Edgar as a co-PI, with Alejandro Ribeiro (PI, ESE), and co-PIs Robert Ghrist (Math), Kostas Daniilidis (CIS), Saswati Sarkar (ESE) July 2019: New paper on data augmentation with Shuxiao Chen and Jane Lee. Summer 2019: Undergraduate researchers Tianle Liu (THU), Shuo Xie (PKU), and Weichen Zheng (Penn) work in the group June-Aug 2019: Edgar is attending the Summer program on the Foundations of Deep Learning at the Simons Institute. March 2019: New paper on distributed learning with Yue Sheng. Miscellanea: I use Twitter to keep up with new research. I grew up in Romania, and speak Hungarian as a first language (the real spelling of my name is Dobribn Edgr). These two countries are and were the origin of many great mathematicians and statisticians, including John von Neumann, Abraham Wald, Paul Erdos, Dan-Virgil Voiculescu, etc Continue Reading Research Talk slides: GitHub. Google Scholar. David Hong, Yue Sheng, Edgar Dobriban, Selecting the number of components in PCA via random signflips.Xiaoxia Wu, Edgar Dobriban, Tongzheng Ren, Shanshan Wu, Zhiyuan Li, Suriya Gunasekar, Rachel Ward, Qiang Liu (2020), Implicit regularization of normalization methods, Neural Information Processing Systems (NeurIPS) 2020.Jonathan Lacotte, Sifan Liu, Edgar Dobriban, Mert Pilanci (2020), Limiting spectrum of randomized hadamard transform and optimal iterative sketching methods, Neural Information Processing Systems (NeurIPS) 2020.Shuxiao Chen, Edgar Dobriban, Jane H Lee (2020), A group-theoretic framework for data augmentation, JMLR, Neural Information Processing Systems (NeurIPS) 2020.Michal Derezinski, Zhenyu Liao, Edgar Dobriban, Michael W. Mahoney, Sparse sketches with small inversion bias.Licong Lin and Edgar Dobriban, What causes the test error? Going beyond bias-variance via ANOVA.Yinjun Wu, Edgar Dobriban, Susan Davidson (2020), DeltaGrad: Rapid retraining of machine learning models, International Conference on Machine Learning (ICML) 2020.Fan Yang, Sifan Liu, Edgar Dobriban, David P. Woodruff, How to reduce dimension with PCA and random projections?.Sifan Liu and Edgar Dobriban (2020), Ridge regression: Structure, cross-validation, and sketching, International Conference on Learning Representations (ICLR).Alnur Ali, Edgar Dobriban, Ryan J. Tibshirani (2020), The implicit regularization of stochastic gradient flow for least squares, International Conference on Machine Learning (ICML) 2020. Teaching Awards and Honors T.W. Anderson Theory of Statistics Dissertation Award, Department of Statistics, Stanford University, 2017Howard Hughes Medical Institute International Student Graduate Research Fellowship, 2015Stanford Department of Statistics Teaching Award, 2013Middleton Miller 29 Prize for best independent work in mathematics, Princeton University, 2012 Miscellaneous This page has links to methods from my papers. Feel free to contact me if you are interested to use them. ePCA: github The ePCA method for principal component analysis of exponential family data, e.g. Poisson-modeled count data. (with L.T. Liu); EigenEdge: github Methods for working with large random data matrices, including Computing eigenvalue distributions of covariance matrices (general Marchenko-Pastur distributions). Optimal statistics for testing in principal component analysis. Tools for spiked covariance models: spike and cosine descriptors, optimal shrinkers. pweight :github R. github Matlab P-value weighting techniques for multiple hypothesis testing. These can improve power in multiple testing, if there is prior information about the individual effect sizes. Includes the iGWAS method for Genome-Wide Association Studies. ActivityLatest ResearchDavid Hong, Yue Sheng, Edgar Dobriban, Selecting the number of components in PCA via random signflips.All Research Additional Links Statistics Department Home Faculty Recruiting Research Programs Resources Department Information Find an Expert Programs Undergraduate MBA EMBA Doctorate Executive Education Wharton Online Locations Philadelphia San Francisco Beijing The Power of Wharton Global Influence Analytics Entrepreneurship & Innovation Featured Give to Wharton Alumni Knowledge@Wharton Recruiters & Corporations Wharton Faculty About Us Research Centers Departments Resources Contact Us News Faculty & Staff TwitterFacebookLinkedInYouTubeInstagramSupport Wharton2020The Wharton School, The University of Pennsylvania | Statistics Department | PrivacyPolicy | ReportAccessibilityIssuesandGetHelp
